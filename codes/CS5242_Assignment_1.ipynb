{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "**ASSIGNMENT DEADLINE: 4 MAR 2018 (SUN) 17:00PM**\n",
    "\n",
    "In this assignment we will be coding the building blocks for the convolutional neural network and putting them together to train a CNN on the MNIST dataset.\n",
    "\n",
    "**Attention: Only python3 will be allowed to use in this assignment. And we use `numpy` to store and caculate data and parameters. You do not need a GPU to for this assignment. CPU is enough. To run this Jupyter notebook, you need to install the depedent libraries in [requiremets.txt](requirements.txt) via pip (or pip3). Note: keras version should be >=2.1.2. Please do not run this whole file before you implement all the codes. Otherwise it will occur some error.**\n",
    "\n",
    "For each layer we will implement a forward and a backward function. The forward function will receive inputs and will return the outputs of this layer(loss layer will be a little different), like this:\n",
    "\n",
    "```python\n",
    "def forward(self, inputs):\n",
    "  \"\"\" Receive inputs and return output\"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  outputs = # the outputs\n",
    "    \n",
    "  return outputs\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and inputs, and will return gradients with respect to the inputs. Gradients for weights or bias will be stored in parameters in this layer , like this:\n",
    "\n",
    "```python\n",
    "def backward(self, in_grads, inputs):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Use values in cache to compute derivatives\n",
    "  out_grads = # Derivative of loss with respect to inputs\n",
    "  self.w_grad = # Derivative of loss with respect to self.weights\n",
    "  self.b_grad = # Derivative of loss with respect to self.bias\n",
    "    \n",
    "  return out_grads\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "\n",
    "This iPython notebook serves to:\n",
    "\n",
    "- explain the questions\n",
    "- explain the function APIs and implementation examples (like `ReLU`) \n",
    "- provide helper functions to piece functions together and check your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer\n",
    "A convolution layer is usually followed by a non-linear activation function. We will provide the functions `forward` and `backward` of class `ReLU` in `layers.py` as an implementation example. Read through the function code and make sure you understand the derivation. Besides, we will explain the implementation of `ReLU` after `Convolution` using formula. You need to write down other layers' formulations in your reports.\n",
    "\n",
    "## Forward Formulation\n",
    "Given input $x \\in R^{B \\times C \\times H \\times W}$ ($B$:batch size, $C$: number of channel, $H$: input height, $W$: input width),  output $y \\in R^{B \\times C \\times H \\times W}$ will be caculated like this:\n",
    "\n",
    "\\begin{equation*}\n",
    "y=indicator(x) \\times x\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $indicator(x)$ return the same size of input $x$, comparing $x$ with 0 element-wisely. If $x_{i,j,k,l} \\geq 0$ return $z_{i,j,k,l}=1$. And the multiplication is also element-wise. If the input $x$ has only 2 dimensions, i.e. the batch dimension and the feature dimension, e.g. after the FC layer, the subscripts $j,k,l$ in the formula are merged into one $j$.\n",
    "\n",
    "## Backward Formulation\n",
    "Given input $x \\in R^{B \\times C \\times H \\times W}$ ($B$:batch size, $C$: number of channel, $H$: input height, $W$: input width) and gradients to output of this layer $dy \\in R^{B \\times C \\times H \\times W}$, gradients to input $dx$ will be caculated like this:\n",
    "\n",
    "\\begin{equation*}\n",
    "dx=indicator(x) \\times dy\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covolution Layer\n",
    "In the file `layers.py`, the class `Convolution` will be initialized with `conv_params`, `initializer` and `name`, shown as below:\n",
    "\n",
    "```python\n",
    "\n",
    "def __init__(self, conv_params, initializer=Guassian(), name='conv'):\n",
    "        super(Convolution, self).__init__(name=name)\n",
    "        self.trainable = True\n",
    "        self.kernel_h = conv_params['kernel_h'] # height of kernel\n",
    "        self.kernel_w = conv_params['kernel_w'] # width of kernel\n",
    "        self.pad = conv_params['pad']\n",
    "        self.stride = conv_params['stride']\n",
    "        self.in_channel = conv_params['in_channel']\n",
    "        self.out_channel = conv_params['out_channel']\n",
    "\n",
    "        self.weights = initializer.initialize((self.out_channel, self.in_channel, self.kernel_h, self.kernel_w))\n",
    "        self.bias = np.zeros((self.out_channel))\n",
    "\n",
    "        self.w_grad = np.zeros(self.weights.shape)\n",
    "        self.b_grad = np.zeros(self.bias.shape)\n",
    "```\n",
    "\n",
    "`conv_params` is a dictionary, containing these parameters:\n",
    "\n",
    "- 'kernel_h': The height of kernel.\n",
    "- 'kernel_w': The width of kernel.\n",
    "- 'stride': The number of pixels between adjacent receptive fields in the horizontal and vertical directions.\n",
    "- 'pad': The number of pixels padded to the bottom, top, left and right of each feature map. **Here, `pad=2` means a 2-pixel border of padded with zeros. So the total number of zeros for horizontal (or vertical) direction is 2\\*pad=4**.\n",
    "- 'in_channel': The number of input channels.\n",
    "- 'out_channel': The number of output channels.\n",
    "\n",
    "`initializer` is an instance of Initializer class (leave it out right now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward\n",
    "In the file `layers.py`, implement the forward pass for a convolutional layer in the function `forward` of class `Convolution`.\n",
    "\n",
    "The input consists of N data points, each with C channels, height H and width W. We convolve each input with K different kernels, where each filter spans all C channels and has height HH and width WW.\n",
    "\n",
    "Input:\n",
    "\n",
    "- inputs: Input data of shape (N, C, H, W)\n",
    "\n",
    "**WARNING:** Please implement the matrix product method of convolution as shown in Lecture notes. The naive version of implementing a sliding window will be too slow when you try to train the whole CNN in later sections.\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers import Convolution\n",
    "from utils.tools import rel_error\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "inputs = np.random.uniform(size=(10, 3, 30, 30))\n",
    "params = { 'kernel_h': 5,\n",
    "          'kernel_w': 5,\n",
    "          'pad': 0,\n",
    "          'stride': 2,\n",
    "          'in_channel': inputs.shape[1],\n",
    "          'out_channel': 64,\n",
    "}\n",
    "layer = Convolution(params)\n",
    "out = layer.forward(inputs)\n",
    "\n",
    "keras_model = keras.Sequential()\n",
    "keras_layer = layers.Conv2D(filters=params['out_channel'],\n",
    "                            kernel_size=(params['kernel_h'], params['kernel_w']),\n",
    "                            strides=(params['stride'], params['stride']),\n",
    "                            padding='valid',\n",
    "                            data_format='channels_first',\n",
    "                            input_shape=inputs.shape[1:])\n",
    "keras_model.add(keras_layer)\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "keras_model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "weights = np.transpose(layer.weights, (2 , 3, 1, 0))\n",
    "keras_layer.set_weights([weights, layer.bias])\n",
    "keras_out = keras_model.predict(inputs, batch_size=inputs.shape[0])\n",
    "print('Relative error (<1e-6 will be fine): ', rel_error(out, keras_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "Implement the backward pass for the convolution operation in the function `backward` of `Convolution` class in the file `layers.py`. \n",
    "\n",
    "When you are done, restart jupyter notebook and run the following to check your backward pass with a numeric gradient check. \n",
    "\n",
    "In gradient checking, to get an approximate gradient for a parameter, we vary that parameter by a small amount (while keeping rest of parameters constant) and note the difference in the network loss. Dividing the difference in network loss by the amount we varied the parameter gives us an approximation for the gradient. We repeat this process for all the other parameters to obtain our numerical gradient. Note that gradient checking is a slow process (2 forward propagations per parameter) and should only be used to check your backpropagation!\n",
    "\n",
    "More links on gradient checking:\n",
    "\n",
    "http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Convolution\n",
    "import numpy as np\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "batch = 10\n",
    "conv_params={\n",
    "    'kernel_h': 3,\n",
    "    'kernel_w': 3,\n",
    "    'pad': 0,\n",
    "    'stride': 2,\n",
    "    'in_channel': 3,\n",
    "    'out_channel': 10\n",
    "}\n",
    "in_height = 10\n",
    "in_width = 20\n",
    "out_height = 1+(in_height+2*conv_params['pad']-conv_params['kernel_h'])//conv_params['stride']\n",
    "out_width = 1+(in_width+2*conv_params['pad']-conv_params['kernel_w'])//conv_params['stride']\n",
    "inputs = np.random.uniform(size=(batch, conv_params['in_channel'], in_height, in_width))\n",
    "in_grads = np.random.uniform(size=(batch, conv_params['out_channel'], out_height, out_width))\n",
    "conv = Convolution(conv_params)\n",
    "check_grads_layer(conv, inputs, in_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Layer\n",
    "Dropout [1] is a technique for regularizing neural networks by randomly setting some features to zero during the forward pass. In this exercise you will implement a dropout layer and modify your fully-connected network to optionally use dropout.\n",
    "\n",
    "[1] Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012\n",
    "\n",
    "In the file `layers.py`, the class `FCLayer` will be initialized with `ratio`, `seed` and `name`, shown as below:\n",
    "```python\n",
    "def __init__(self, ratio, name='dropout', seed=None):\n",
    "        super(Dropout, self).__init__(name=name)\n",
    "        self.ratio = ratio\n",
    "        self.mask = None\n",
    "        self.seed = seed\n",
    "```\n",
    "\n",
    "- `ratio`: The probability of setting a neuron to zero\n",
    "- `seed`: Random seed to sample from inputs, so as to get mask. (default as None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward\n",
    "In the file `layers.py`, implement the forward pass for dropout. Since dropout behaves differently during training and testing, make sure to implement the operation for both modes.  `p` refers to the probability of setting a neuron to zero. We will follow the Caffe convention where we multiply the outputs by `1/(1-p)` during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "In the file `layers.py`, implement the backward pass for dropout. After doing so, restart jupyter notebook and run the following cell to numerically gradient-check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Dropout\n",
    "import numpy as np\n",
    "from utils.check_grads import check_grads_layer\n",
    "batch=10\n",
    "ratio = 0.1\n",
    "height = 10\n",
    "width = 20\n",
    "channel = 10\n",
    "np.random.seed(1234)\n",
    "inputs = np.random.uniform(size=(batch, channel, height, width))\n",
    "in_grads = np.random.uniform(size=(batch, channel, height, width))\n",
    "dropout = Dropout(ratio, seed=1234)\n",
    "dropout.set_mode(True)\n",
    "check_grads_layer(dropout, inputs, in_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling Layer\n",
    "In the file `layers.py`, the class `Pooling` will be initialized with `pool_params`, and `name`, shown as below:\n",
    "```python\n",
    "def __init__(self, pool_params, name='pooling'):\n",
    "        super(Pooling, self).__init__(name=name)\n",
    "        self.pool_type = pool_params['pool_type']\n",
    "        self.pool_height = pool_params['pool_height']\n",
    "        self.pool_width = pool_params['pool_width']\n",
    "        self.stride = pool_params['stride']\n",
    "        self.pad = pool_params['pad']\n",
    "```\n",
    "\n",
    "`pool_params` is a dictionary, containing these parameters:\n",
    "\n",
    "- 'pool_type': The type of pooling, 'max' or 'avg'\n",
    "- 'pool_h': The height of pooling kernel.\n",
    "- 'pool_w': The width of pooling kernel.\n",
    "- 'stride': The number of pixels between adjacent receptive fields in the horizontal and vertical directions.\n",
    "- 'pad': The number of pixels that will be used to zero-pad the input in each x-y direction. **Here, `pad=2` means a 2-pixel border of padding with zeros**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward\n",
    "Implement the forward pass for the pooling operation in the function `forward` of class `Pooling` in the file `layers.py`.\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers import Pooling\n",
    "from utils.tools import rel_error\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "inputs = np.random.uniform(size=(10, 3, 30, 30))\n",
    "params = { 'pool_type': 'max',\n",
    "           'pool_height': 5,\n",
    "           'pool_width': 5,\n",
    "           'pad': 0,\n",
    "           'stride': 2,\n",
    "}\n",
    "layer = Pooling(params)\n",
    "out = layer.forward(inputs)\n",
    "\n",
    "keras_model = keras.Sequential()\n",
    "keras_layer = layers.MaxPooling2D(pool_size=(params['pool_height'], params['pool_width']),\n",
    "                                 strides=params['stride'],\n",
    "                                 padding='valid',\n",
    "                                 data_format='channels_first',\n",
    "                                 input_shape=inputs.shape[1:])\n",
    "keras_model.add(keras_layer)\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "keras_model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "keras_out = keras_model.predict(inputs, batch_size=inputs.shape[0])\n",
    "print('Relative error (<1e-6 will be fine): ', rel_error(out, keras_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backward\n",
    "#Implement the backward pass for the max-pooling operation in the function `backward` of class `Pooling` in the file `layers.py`.\n",
    "#Please make sure you have implemented both ’max’ and ’avg’ pooing in your codes. And then test the gradients by yourself.\n",
    "\n",
    "from layers import Pooling\n",
    "import numpy as np\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "params = { 'pool_type': 'max',\n",
    "           'pool_height': 5,\n",
    "           'pool_width': 5,\n",
    "           'pad': 0,\n",
    "           'stride': 2,\n",
    "}\n",
    "in_height = 10\n",
    "in_width = 20\n",
    "out_height = 1+(in_height+2*params['pad']-params['pool_height'])//params['stride']\n",
    "out_width = 1+(in_width+2*params['pad']-params['pool_width'])//params['stride']\n",
    "inputs = np.random.uniform(size=(10, 3, in_height, in_width))\n",
    "in_grads = np.random.uniform(size=(10, 3, out_height, out_width))\n",
    "pool = Pooling(params)\n",
    "check_grads_layer(pool, inputs, in_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FC Layer\n",
    "\n",
    "FC layer (short for fully connected layer) is also called linear layer or dense layer.\n",
    "\n",
    "In the file `layers.py`, the class `FCLayer` will be initialized with `in_features`, `out_features`, and `name`, shown as below:\n",
    "```python\n",
    "def __init__(self, in_features, out_features, name='fclayer', initializer=Guassian()):\n",
    "        super(FCLayer, self).__init__(name=name)\n",
    "        self.trainable = True\n",
    "        self.weights = initializer.initialize((in_features, out_features))\n",
    "        self.bias = initializer.initialize(out_features)\n",
    "\n",
    "        self.w_grad = np.zeros(self.weights.shape)\n",
    "        self.b_grad = np.zeros(self.bias.shape)\n",
    "```\n",
    "\n",
    "- `in_features`: The number of inputs features\n",
    "- `out_features`: The numbet of required outputs features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward\n",
    "Implement the forward pass for the pooling operation in the function `forward` of class `FCLayer` in the file `layers.py`.\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers import FCLayer\n",
    "from utils.tools import rel_error\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "inputs = np.random.uniform(size=(10, 20))\n",
    "\n",
    "layer = FCLayer(in_features=inputs.shape[1], out_features=100)\n",
    "out = layer.forward(inputs)\n",
    "\n",
    "keras_model = keras.Sequential()\n",
    "keras_layer = layers.Dense(100, input_shape=inputs.shape[1:], use_bias=True, kernel_initializer='random_normal', bias_initializer='zeros')\n",
    "# print (len(keras_layer.get_weights()))\n",
    "keras_model.add(keras_layer)\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "keras_model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "keras_layer.set_weights([layer.weights, layer.bias])\n",
    "keras_out = keras_model.predict(inputs, batch_size=inputs.shape[0])\n",
    "print('Relative error (<1e-6 will be fine): ', rel_error(out, keras_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backward\n",
    "#Implement the backward pass for the max-pooling operation in the function `backward` of class `FCLayer` in the file `layers.py`. Please test the gradients by yourself.\n",
    "\n",
    "from layers import FCLayer\n",
    "import numpy as np\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "inputs = np.random.uniform(size=(10, 20));\n",
    "in_grads = np.random.uniform(size=(10, 100))\n",
    "fclayer = FCLayer(in_features=20, out_features=100)\n",
    "check_grads_layer(fclayer, inputs, in_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftmaxCrossEntropy Loss\n",
    "We write Softmax and CrossEntropy together because it can avoid some numeric overflow problem.In the file `loss.py`, the class `SoftmaxCrossEntropy` will be initialized with `num_class`,  shown as below:\n",
    "```python\n",
    "def __init__(self, num_class):\n",
    "        super(SoftmaxCrossEntropy, self).__init__()\n",
    "        self.num_class = num_class\n",
    "```\n",
    "\n",
    "`num_class`: The number of category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward\n",
    "Implement the forward pass for the pooling operation in the function `forward` of class `FCLayer` in the file `layers.py`.\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from loss import SoftmaxCrossEntropy\n",
    "from utils.tools import rel_error\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "batch = 10\n",
    "num_class = 10\n",
    "inputs = np.random.uniform(size=(batch, num_class))\n",
    "targets = np.random.randint(num_class, size=batch)\n",
    "\n",
    "loss = SoftmaxCrossEntropy(num_class)\n",
    "out, _ = loss.forward(inputs, targets)\n",
    "\n",
    "keras_inputs = K.softmax(inputs)\n",
    "keras_targets = np.zeros(inputs.shape, dtype='int')\n",
    "for i in range(batch):\n",
    "        keras_targets[i, targets[i]] = 1\n",
    "keras_out = K.mean(K.categorical_crossentropy(keras_targets, keras_inputs, from_logits=False))\n",
    "print(out)\n",
    "print(K.eval(keras_out))\n",
    "print('Relative error (<1e-6 will be fine): ', rel_error(out, K.eval(keras_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backward\n",
    "# In the file `loss.py`, implement the backward pass for `SoftmaxCrossEntropy`. Please test the gradients by yourself.\n",
    "\n",
    "import numpy as np\n",
    "from loss import SoftmaxCrossEntropy\n",
    "from utils.check_grads import check_grads_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "batch = 10\n",
    "num_class = 10\n",
    "inputs = np.random.uniform(size=(batch, num_class))\n",
    "targets = np.random.randint(num_class, size=batch)\n",
    "loss = SoftmaxCrossEntropy(num_class)\n",
    "check_grads_loss(loss, inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "In the file `optimizers.py`, there are 4 types of optimizer (`SGD`, `Adam`, `RMSprop` and `Adagrad`). You only need to implement the `update` function of `SGD`(mini-batch SGD with momentum) and `Adam`. These two types of optimizers are initialized like this:\n",
    "\n",
    "```python\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr=0.01, momentum=0, decay=0, sheduler_func = None):\n",
    "        super(SGD, self).__init__(lr)\n",
    "        self.momentum = momentum\n",
    "        self.moments = None\n",
    "        self.decay = decay\n",
    "        self.sheduler_func = sheduler_func\n",
    "        \n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, sheduler_func=None):\n",
    "        super(Adam, self).__init__(lr)\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        if not self.epsilon:\n",
    "            self.epsilon = 1e-8\n",
    "        self.moments = None\n",
    "        self.accumulators = None\n",
    "        self.sheduler_func = sheduler_func\n",
    "```\n",
    "\n",
    "For Both optimizers:\n",
    "- `lr`: The initial learning rate.\n",
    "- `decay`: The learning rate decay ratio\n",
    "- `sheduler_func`: Function to change learning rate with respect to iterations\n",
    "\n",
    "For `SGD`:\n",
    "- `momentum`: The ratio of moments\n",
    "\n",
    "\n",
    "For `Adam`:\n",
    "More details can be seen in reference.\n",
    "\n",
    "**For you reference:**\n",
    "http://cs231n.github.io/neural-networks-3/#update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the net on full MNIST data\n",
    "By training the `MNISTNet` for one epoch, you should achieve about 90% on the validation and test set. You may have to wait about 5 minutes for training to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  48000\n",
      "Number of validation images:  12000\n",
      "Number of testing images:  10000\n",
      "\n",
      "Four examples of training images:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12efcd160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1100921d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from applications import MNISTNet\n",
    "from loss import SoftmaxCrossEntropy, L2\n",
    "from optimizers import Adagrad, Adam, RMSprop\n",
    "from utils.datsets import MNIST\n",
    "import numpy as np\n",
    "\n",
    "mnist = MNIST()\n",
    "mnist.load()\n",
    "idx = np.random.randint(mnist.num_train, size=4)\n",
    "print('\\nFour examples of training images:')\n",
    "img = mnist.x_train[idx][:,0,:,:]\n",
    "\n",
    "plt.figure(1, figsize=(18, 18))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(img[0])\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(img[1])\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(img[2])\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(img[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: \n",
      "Test accuracy=0.10080, loss=2.30259\n",
      "Validation accuracy: 0.09508, loss: 2.30259\n",
      "Iteration 0:\taccuracy=0.20000, loss=2.30259, regularization loss= 0.005287661552302978\n",
      "Validation accuracy: 0.10600, loss: 2.30234\n",
      "Iteration 100:\taccuracy=0.20000, loss=2.30018, regularization loss= 0.0019655938426539456\n",
      "Validation accuracy: 0.10600, loss: 2.30212\n",
      "Iteration 200:\taccuracy=0.16667, loss=2.29855, regularization loss= 0.0007510983981031626\n",
      "Test accuracy=0.11350, loss=2.30178\n",
      "Validation accuracy: 0.10600, loss: 2.30213\n",
      "Iteration 300:\taccuracy=0.10000, loss=2.30357, regularization loss= 0.0002991660250718179\n",
      "Validation accuracy: 0.10600, loss: 2.30197\n",
      "Iteration 400:\taccuracy=0.13333, loss=2.29274, regularization loss= 0.00015351510418540367\n",
      "Validation accuracy: 0.10600, loss: 2.29958\n",
      "Iteration 500:\taccuracy=0.03333, loss=2.29979, regularization loss= 0.0002636141375401724\n",
      "Test accuracy=0.34440, loss=1.88961\n",
      "Validation accuracy: 0.33350, loss: 1.91060\n",
      "Iteration 600:\taccuracy=0.30000, loss=1.88605, regularization loss= 0.004334052678146069\n",
      "Validation accuracy: 0.50475, loss: 1.33853\n",
      "Iteration 700:\taccuracy=0.53333, loss=1.12403, regularization loss= 0.008576215295941528\n",
      "Validation accuracy: 0.64167, loss: 1.03685\n",
      "Iteration 800:\taccuracy=0.53333, loss=0.93446, regularization loss= 0.010472063568393395\n",
      "Test accuracy=0.68190, loss=0.88655\n",
      "Validation accuracy: 0.69075, loss: 0.88072\n",
      "Iteration 900:\taccuracy=0.76667, loss=0.74512, regularization loss= 0.011218355663140148\n",
      "Validation accuracy: 0.73500, loss: 0.75934\n",
      "Iteration 1000:\taccuracy=0.70000, loss=0.73389, regularization loss= 0.012052029862253079\n",
      "Validation accuracy: 0.75600, loss: 0.73279\n",
      "Iteration 1100:\taccuracy=0.86667, loss=0.31278, regularization loss= 0.012602305369162748\n",
      "Test accuracy=0.77380, loss=0.68491\n",
      "Validation accuracy: 0.77875, loss: 0.67469\n",
      "Iteration 1200:\taccuracy=0.76667, loss=0.77085, regularization loss= 0.013058770465656162\n",
      "Validation accuracy: 0.80517, loss: 0.62290\n",
      "Iteration 1300:\taccuracy=0.83333, loss=0.46649, regularization loss= 0.01353131106114585\n",
      "Validation accuracy: 0.79975, loss: 0.60914\n",
      "Iteration 1400:\taccuracy=0.63333, loss=1.63959, regularization loss= 0.013949725547566523\n",
      "Test accuracy=0.80560, loss=0.59297\n",
      "Validation accuracy: 0.81025, loss: 0.58385\n",
      "Iteration 1500:\taccuracy=0.80000, loss=0.55619, regularization loss= 0.014490389573988106\n",
      "Epoch 1: \n",
      "Test accuracy=0.82310, loss=0.56509\n",
      "Validation accuracy: 0.82675, loss: 0.55787\n",
      "Iteration 0:\taccuracy=0.70000, loss=1.02564, regularization loss= 0.014906207032080544\n",
      "Validation accuracy: 0.83000, loss: 0.52767\n",
      "Iteration 100:\taccuracy=0.70000, loss=0.76942, regularization loss= 0.0154186336873474\n",
      "Validation accuracy: 0.83550, loss: 0.50297\n",
      "Iteration 200:\taccuracy=0.93333, loss=0.21912, regularization loss= 0.015964132960414955\n",
      "Test accuracy=0.82190, loss=0.53906\n",
      "Validation accuracy: 0.82750, loss: 0.52706\n",
      "Iteration 300:\taccuracy=0.80000, loss=0.59514, regularization loss= 0.01645989742728561\n"
     ]
    }
   ],
   "source": [
    "model = MNISTNet()\n",
    "loss = SoftmaxCrossEntropy(num_class=10)\n",
    "\n",
    "# define your learning rate sheduler\n",
    "def func(lr, iteration):\n",
    "    if iteration % 1000 ==0:\n",
    "        return lr*0.5\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "adam = RMSprop(lr=0.001, decay=0,  sheduler_func = func)\n",
    "l2 = L2(w=0.001) # L2 regularization with lambda=0.001\n",
    "model.compile(optimizer=adam, loss=loss, regularization=l2)\n",
    "train_results, val_results, test_results = model.train(\n",
    "    mnist, \n",
    "    train_batch=30, val_batch=1000, test_batch=1000, \n",
    "    epochs=2, \n",
    "    val_intervals=100, test_intervals=300, print_intervals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(18, 8))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(train_results[:,0], train_results[:,1])\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title('Training accuracy')\n",
    "plt.plot(train_results[:,0], train_results[:,2])\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title('Validation loss')\n",
    "plt.plot(val_results[:,0], val_results[:,1])\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title('Validation accuracy')\n",
    "plt.plot(val_results[:,0], val_results[:,2])\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title('Testing loss')\n",
    "plt.plot(test_results[:,0], test_results[:, 1])\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.title('Testing accuracy')\n",
    "plt.plot(test_results[:, 0], test_results[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change of learning rate\n",
    "If we change the initial learning rate from 0.001 to 0.1, the training process becomes unstable and the loss is out of control. Thus, you need to be careful when setting the initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTNet()\n",
    "loss = SoftmaxCrossEntropy(num_class=10)\n",
    "\n",
    "# define your learning rate sheduler\n",
    "def func(lr, iteration):\n",
    "    if iteration % 1000 ==0:\n",
    "        return lr*0.5\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "adam = Adam(lr=0.01, decay=0,  sheduler_func = func)\n",
    "l2 = L2(w=0.001) # L2 regularization with lambda=0.001\n",
    "model.compile(optimizer=adam, loss=loss, regularization=l2)\n",
    "train_results, val_results, test_results = model.train(\n",
    "    mnist, \n",
    "    train_batch=30, val_batch=1000, test_batch=1000, \n",
    "    epochs=2, \n",
    "    val_intervals=100, test_intervals=300, print_intervals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(18, 8))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(train_results[:,0], train_results[:,1])\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title('Training accuracy')\n",
    "plt.plot(train_results[:,0], train_results[:,2])\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title('Validation loss')\n",
    "plt.plot(val_results[:,0], val_results[:,1])\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title('Validation accuracy')\n",
    "plt.plot(val_results[:,0], val_results[:,2])\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title('Testing loss')\n",
    "plt.plot(test_results[:,0], test_results[:, 1])\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.title('Testing accuracy')\n",
    "plt.plot(test_results[:, 0], test_results[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your best MNISTNet!\n",
    "Tweak the hyperparameters of the above MNISTNet and use what you've learnt to train the best net.\n",
    "\n",
    "Credits will be given based on your test accuracy, your explanations/insights of the training. The network is small, hence the training should finish quickly using your CPU (less than 1 hour). \n",
    "\n",
    "Please report the following:\n",
    "- Training validation and testing loss as well as accuracy over iterations\n",
    "- Architecture and training method (eg. optimization scheme, data augmentation): explain your design choices, what has failed and what has worked and why you think they worked/failed\n",
    "- Try different hyper-parameters, e.g. rate decaying, weight decay, number of layers and total number of epochs. \n",
    "\n",
    "Do NOT use external libraries like Tensorflow, keras and Pytorch in your implementation, i.e. optimizer.py, layer.py and loss.py. Do NOT copy the code from the internet, e.g. github. You should also give credits to any material that you refer to for your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Final submission instructions\n",
    "Please submit the following:\n",
    "\n",
    "1) Your code files in a folder `codes`\n",
    "\n",
    "2) A short report (1-2 pages) in pdf titled `report.pdf`, explaining the logic (expressed using mathematical formulation) of your implementation (including the forward and backward function like ReLU) and the findings from training your best net\n",
    "\n",
    "**ASSIGNMENT DEADLINE: 4 MAR 2018 (SUN) 17:00PM**\n",
    "\n",
    "Do not include the `data` folder as it takes up substantial memory. Please zip up the following folders under a folder named with your NUSNET ID: eg. `e0123456g.zip' and submit the zipped folder to IVLE/workbin/assignment 1 submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
